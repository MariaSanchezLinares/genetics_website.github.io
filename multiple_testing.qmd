---
title: "Multiple Testing: Problems and Solutions"
format:
  html:
    include-in-header: 
      text: |
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
---

## The Problem with Multiple Testing

When performing a large number of hypothesis tests, the likelihood of encountering **false positives** (Type I errors) increases. To address this, we aim to **control for multiple testing** by maintaining the error rate at an acceptable level. Two commonly used error rates are:

-   **Family-Wise Error Rate (FWER):** The probability of making at least one Type I error across all tests.\
    $$
    \text{FWER} = P(\text{at least one Type I error})
    $$

    A common threshold is 0.05.

-   **False Discovery Rate (FDR):** The expected proportion of false discoveries (false positives) among all significant results.\
    $$
    \text{FDR} = E \left[ \frac{FP}{FP + TP} \right]
    $$

    A common threshold is also 0.05.

In both cases, we need to adjust our significance threshold to account for the number of tests conducted.

## Solutions to Multiple Testing

#### **1. Bonferroni Correction**

One widely used approach is the **Bonferroni Correction**, which adjusts the significance threshold by dividing the desired FWER by the number of tests:

$$
\alpha_{\text{Bonferroni}} = \frac{\text{FWER}}{\text{# of tests}}
$$

For example, if we conduct **1 million tests** and want to control the FWER at **5%**, our new significance threshold would be:

$$\alpha_{\text{Bonferroni}} = \frac{0.05}{1,000,000} = 5 \times 10^{-8}$$

**Pros:** Simple and easy to calculate

**Cons:** Can be overly conservative, especially when tests are correlated (which is often the case in GWAS), reducing power and increasing Type II errors.

#### **2. Simulation-Based Approach**

Another way to control for multiple testing is through **simulation-based methods**, such as those described in the paper we read for Journal Club 3.

**Procedure:**

1.  Simulate a **null trait** (a trait with no association to any SNPs).
2.  Perform GWAS on this null trait.
3.  Record the smallest **p-value** from this GWAS.
4.  Repeat steps 1–3 many times (e.g., **100–1,000 times**).
5.  Rank the simulated **p-values** and determine the **5th percentile** threshold.

**Pros:** Works even when tests are correlated

**Cons:** Computationally expensive

## Independent vs. Correlated Tests

#### **Independent Tests**

When tests are independent, the probability of at least one **Type I error** follows:

$$\text{FWER} = 1 - [1 - \alpha]^{\text{# of tests}}$$

To control FWER, we need a smaller significance threshold. Both **Bonferroni** and **Simulation-Based** approaches work well in this setting and yield similar results.

#### **Correlated Tests**

When tests are correlated, the two methods can yield very different thresholds:

-   **Bonferroni is too conservative**, leading to more Type II errors and lower power.
-   **A better approach** is to adjust for the **effective number of independent tests**, rather than the total number of tests.

**Potential Project Idea:** Explore alternative methods for multiple testing correction beyond Bonferroni and simulation-based approaches.

::: {.callout-tip title="Key Takeaway"}
By choosing the right multiple testing correction method, we balance **Type I errors (false positives)** and **Type II errors (false negatives)** while maintaining statistical power.
:::
